{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Semi-supervision for Image Classification </h1>\n",
    "Grant Sinha, gsinha@uwaterloo.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Abstract </h2>\n",
    "\n",
    "The below approach combines supervised learning (via cross-entropy loss for labeled data) and unsupervised learning (via a K-means clustering loss for unlabeled data). These are combined as a weighted linear combination for the final loss function. In this way, unlabaled data will guide feature extraction and refine cluster boundaries in the feature space beyond what would be the case if only using the labeled portion. \n",
    "\n",
    "We will use the MNIST dataset for its simplicity and ease of use. We assume M samples in the training set are labeled, treating the rest of the training set as unlabeled. We will define a convolutional network to output a fairly small (64, 7, 7)-shaped feature map, which is flattened and fed through a linear layer before classification. \n",
    "\n",
    "We also investigate the effect of an additional loss term for consistency regularization. Specifically, we apply a small, random transformation to training data and encourage the model's features for the augmented image to remain close to the unaugmented image's features.\n",
    "\n",
    "To benchmark performance, we measure accuracy on a test split. We use confusion matrices in order to further probe these results and identify model errors. Finally, to investigate the feature space, we create tSNE plots. Analyzing how these vary across experiments will give us the desired insight into effects of K-means and consistency regularization. Ultimately, these will lead to higher accuracy and more discriminative features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Libraries & Environment Setup </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os  # Environment management\n",
    "import numpy as np  # Numpy\n",
    "import random  # Determinism\n",
    "import torch  # PyTorch\n",
    "import torch.nn as nn  # Neural network module\n",
    "import torch.optim as optim  # Optimizers\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import matplotlib.animation as animation # Animation\n",
    "import warnings  # Silence some sklearn warnings\n",
    "import optuna # Hyperparameter optimization\n",
    "\n",
    "from collections import Counter  # Counting\n",
    "from torchvision import datasets, transforms  # Datasets and transformations\n",
    "from sklearn.cluster import KMeans  # KMeans clustering algorithm\n",
    "from sklearn.model_selection import train_test_split  # Train-test split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # Confusion matrix\n",
    "from sklearn.manifold import TSNE  # t-SNE\n",
    "from tqdm.notebook import tqdm  # Progress bars\n",
    "\n",
    "from models import CNN  # Neural network class\n",
    "from trainers import KMeansConsistencyTrainer  # Training function\n",
    "from datasets import get_mnist_loaders, split_dataset  # Dataset functions\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")    # Silence some annoying sklearn warnings\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Due to sklearn bug\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seeds for determinism\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> MNIST Data Preparation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "train_loader, test_loader = get_mnist_loaders(batch_size=128)\n",
    "\n",
    "# Split dataset into labeled and unlabeled subsets\n",
    "mnist_train = train_loader.dataset\n",
    "# labeled_data, unlabeled_data = split_dataset(mnist_train, num_labeled=250)\n",
    "\n",
    "# Create data loaders for labeled and unlabeled data\n",
    "# labeled_loader = torch.utils.data.DataLoader(labeled_data, batch_size=128, shuffle=True, num_workers=0)\n",
    "# unlabeled_loader = torch.utils.data.DataLoader(unlabeled_data, batch_size=128, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Training Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment setup\n",
    "def experiment(values_of_M, epochs=10, evaluate_every=1, lambda_kmeans=0.1, lambda_consistency=0.1, use_consistency=False, use_unlabeled=True, save_dir=\"tsne_images\", output_file=\"tsne_animation.gif\", generate_tsne=True, generate_cm=True):\n",
    "    results = {}\n",
    "    for M in values_of_M:\n",
    "        # Create model instance\n",
    "        model = CNN(use_dropout = True, dropout_rate = 0.3)\n",
    "        trainer = KMeansConsistencyTrainer(model, device=device)\n",
    "\n",
    "        # Split dataset\n",
    "        labeled_data, unlabeled_data = split_dataset(mnist_train, M)\n",
    "        # print_label_distribution(labeled_data, description=f\"Labeled Dataset for M={M}\")\n",
    "        \n",
    "        labeled_loader = torch.utils.data.DataLoader(labeled_data, batch_size=256, shuffle=True, num_workers=0)\n",
    "        unlabeled_loader = torch.utils.data.DataLoader(unlabeled_data, batch_size=256, shuffle=True, num_workers=0) if use_unlabeled else None\n",
    "\n",
    "        # Train and log results\n",
    "        train_accs, test_accs = trainer.train(\n",
    "            labeled_loader=labeled_loader, \n",
    "            unlabeled_loader=unlabeled_loader, \n",
    "            test_loader=test_loader, \n",
    "            epochs=epochs, \n",
    "            evaluate_every=evaluate_every\n",
    "        )\n",
    "        results[M] = (train_accs, test_accs)\n",
    "        # print(f\"Train accuracy for M={M}: {train_accs[-1]:.2f}%, Test accuracy for M={M}: {test_accs[-1]:.2f}%\")\n",
    "\n",
    "\n",
    "        # Evaluate and compute confusion matrix\n",
    "        test_accuracy, preds, labels = trainer.evaluate(test_loader)\n",
    "        if generate_cm:\n",
    "            cm = confusion_matrix(labels, preds)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(10))\n",
    "            disp.plot(cmap=\"viridis\")\n",
    "            plt.title(f\"Confusion Matrix for M={M}\")\n",
    "            plt.show()\n",
    "\n",
    "        if generate_tsne:\n",
    "            # Plot t-SNE visualization\n",
    "            plot_tsne(model, M, test_loader)\n",
    "\n",
    "    if generate_tsne:\n",
    "        create_animation(save_dir, output_file)\n",
    "    return results\n",
    "\n",
    "# Plot training curves\n",
    "def plot_training_curves(results, values_of_M):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot test accuracy for each M\n",
    "    for M in values_of_M:\n",
    "        train_accs, test_accs = results[M]\n",
    "        plt.plot(test_accs, label=f\"M={M} (Test Accuracy)\")\n",
    "\n",
    "    plt.title(\"Test Accuracy vs. Epochs for Different Values of M\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Test Accuracy (%)\")\n",
    "    plt.ylim(0, 100)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Plot t-SNE visualization\n",
    "def plot_tsne(model, M, data_loader, save_dir=\"tsne_images\"):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            _, features = model(images)\n",
    "            features_list.append(features.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "    features = np.concatenate(features_list)\n",
    "    labels = np.concatenate(labels_list)\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    tsne_results = tsne.fit_transform(features)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(f't-SNE Visualization for M={M}')\n",
    "    plt.savefig(os.path.join(save_dir, f'tsne_M_{M}.png'))\n",
    "    plt.show()  # Display the plot in the output\n",
    "    plt.close()\n",
    "\n",
    "def create_animation(save_dir=\"tsne_images\", output_file=\"tsne_animation.gif\"):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    images = []\n",
    "    \n",
    "    # Extract numeric values of M from filenames and sort them\n",
    "    file_names = sorted(\n",
    "        [f for f in os.listdir(save_dir) if f.startswith('tsne_M_') and f.endswith('.png')],\n",
    "        key=lambda x: int(x.split('_')[2].split('.')[0])\n",
    "    )\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        img = plt.imread(os.path.join(save_dir, file_name))\n",
    "        images.append([plt.imshow(img, animated=True)])\n",
    "    \n",
    "    ani = animation.ArtistAnimation(fig, images, interval=500, blit=True, repeat_delay=1000)\n",
    "    ani.save(os.path.join(save_dir, output_file), writer='imagemagick')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Hyperparameter Optimization with Optuna </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-23 16:52:59,068] A new study created in memory with name: Optimize_KMeans_and_Consistency_Losses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.3870286345481873, Train Accuracy: 15.70%, Test Accuracy: 16.89%\n",
      "Epoch [2/10], Loss: 2.0041126012802124, Train Accuracy: 32.20%, Test Accuracy: 68.43%\n",
      "Epoch [3/10], Loss: 1.5176301002502441, Train Accuracy: 66.80%, Test Accuracy: 68.43%\n",
      "Epoch [4/10], Loss: 0.9843920767307281, Train Accuracy: 77.70%, Test Accuracy: 80.67%\n",
      "Epoch [5/10], Loss: 0.6774343550205231, Train Accuracy: 80.80%, Test Accuracy: 80.67%\n",
      "Epoch [6/10], Loss: 0.5431683361530304, Train Accuracy: 83.50%, Test Accuracy: 85.77%\n",
      "Epoch [7/10], Loss: 0.5054378882050514, Train Accuracy: 84.70%, Test Accuracy: 85.77%\n",
      "Epoch [8/10], Loss: 0.49727731943130493, Train Accuracy: 84.40%, Test Accuracy: 86.18%\n",
      "Epoch [9/10], Loss: 0.4702412709593773, Train Accuracy: 85.00%, Test Accuracy: 86.18%\n",
      "Epoch [10/10], Loss: 0.4555510953068733, Train Accuracy: 85.80%, Test Accuracy: 87.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-23 16:54:47,091] Trial 0 finished with value: 87.69 and parameters: {'lambda_kmeans': 0.0010337871850909183, 'lambda_consistency': 8.34959100047629e-06}. Best is trial 0 with value: 87.69.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.210574448108673, Train Accuracy: 25.20%, Test Accuracy: 39.83%\n",
      "Epoch [2/10], Loss: 1.7021483480930328, Train Accuracy: 55.60%, Test Accuracy: 80.71%\n",
      "Epoch [3/10], Loss: 1.061434656381607, Train Accuracy: 74.30%, Test Accuracy: 80.71%\n",
      "Epoch [4/10], Loss: 0.6843858659267426, Train Accuracy: 79.70%, Test Accuracy: 84.37%\n",
      "Epoch [5/10], Loss: 0.5040683075785637, Train Accuracy: 83.80%, Test Accuracy: 84.37%\n",
      "Epoch [6/10], Loss: 0.4279017448425293, Train Accuracy: 86.50%, Test Accuracy: 88.20%\n",
      "Epoch [7/10], Loss: 0.4005499705672264, Train Accuracy: 87.50%, Test Accuracy: 88.20%\n",
      "Epoch [8/10], Loss: 0.3867444470524788, Train Accuracy: 87.80%, Test Accuracy: 88.41%\n",
      "Epoch [9/10], Loss: 0.3868577480316162, Train Accuracy: 87.70%, Test Accuracy: 88.41%\n",
      "Epoch [10/10], Loss: 0.3716476932168007, Train Accuracy: 88.70%, Test Accuracy: 89.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-23 16:56:28,086] Trial 1 finished with value: 89.34 and parameters: {'lambda_kmeans': 1.758538400114545e-05, 'lambda_consistency': 0.00013983372460331177}. Best is trial 1 with value: 89.34.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.2314908504486084, Train Accuracy: 13.90%, Test Accuracy: 60.80%\n",
      "Epoch [2/10], Loss: 1.729572206735611, Train Accuracy: 59.60%, Test Accuracy: 73.15%\n",
      "Epoch [3/10], Loss: 1.0963037461042404, Train Accuracy: 75.30%, Test Accuracy: 73.15%\n",
      "Epoch [4/10], Loss: 0.6987478137016296, Train Accuracy: 80.60%, Test Accuracy: 83.22%\n",
      "Epoch [5/10], Loss: 0.519818589091301, Train Accuracy: 82.60%, Test Accuracy: 83.22%\n",
      "Epoch [6/10], Loss: 0.41649414598941803, Train Accuracy: 86.70%, Test Accuracy: 88.21%\n",
      "Epoch [7/10], Loss: 0.39179813116788864, Train Accuracy: 87.20%, Test Accuracy: 88.21%\n",
      "Epoch [8/10], Loss: 0.39184504747390747, Train Accuracy: 86.60%, Test Accuracy: 88.57%\n",
      "Epoch [9/10], Loss: 0.3639788553118706, Train Accuracy: 88.80%, Test Accuracy: 88.57%\n",
      "Epoch [10/10], Loss: 0.3747972697019577, Train Accuracy: 88.80%, Test Accuracy: 89.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-23 16:58:08,109] Trial 2 finished with value: 89.46 and parameters: {'lambda_kmeans': 1.1602777659091768e-05, 'lambda_consistency': 0.0036811660066349105}. Best is trial 2 with value: 89.46.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.232264280319214, Train Accuracy: 21.70%, Test Accuracy: 45.28%\n",
      "Epoch [2/10], Loss: 1.7366627752780914, Train Accuracy: 55.00%, Test Accuracy: 74.08%\n",
      "Epoch [3/10], Loss: 1.113545373082161, Train Accuracy: 73.00%, Test Accuracy: 74.08%\n",
      "Epoch [4/10], Loss: 0.7254999876022339, Train Accuracy: 80.60%, Test Accuracy: 83.27%\n",
      "Epoch [5/10], Loss: 0.5456205978989601, Train Accuracy: 81.60%, Test Accuracy: 83.27%\n",
      "Epoch [6/10], Loss: 0.44455868005752563, Train Accuracy: 85.10%, Test Accuracy: 87.32%\n",
      "Epoch [7/10], Loss: 0.42366183549165726, Train Accuracy: 86.20%, Test Accuracy: 87.32%\n",
      "Epoch [8/10], Loss: 0.4118703156709671, Train Accuracy: 87.00%, Test Accuracy: 88.35%\n",
      "Epoch [9/10], Loss: 0.3888649120926857, Train Accuracy: 87.10%, Test Accuracy: 88.35%\n",
      "Epoch [10/10], Loss: 0.38151922821998596, Train Accuracy: 87.30%, Test Accuracy: 88.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-23 16:59:49,075] Trial 3 finished with value: 88.66 and parameters: {'lambda_kmeans': 0.0008378883008109513, 'lambda_consistency': 0.0026401803200526825}. Best is trial 2 with value: 89.46.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.2831152081489563, Train Accuracy: 19.70%, Test Accuracy: 30.16%\n",
      "Epoch [2/10], Loss: 1.875045359134674, Train Accuracy: 47.20%, Test Accuracy: 72.95%\n",
      "Epoch [3/10], Loss: 1.305981159210205, Train Accuracy: 75.40%, Test Accuracy: 72.95%\n",
      "Epoch [4/10], Loss: 0.86561319231987, Train Accuracy: 75.80%, Test Accuracy: 81.49%\n",
      "Epoch [5/10], Loss: 0.6176142245531082, Train Accuracy: 80.20%, Test Accuracy: 81.49%\n",
      "Epoch [6/10], Loss: 0.5113637521862984, Train Accuracy: 83.10%, Test Accuracy: 85.43%\n",
      "Epoch [7/10], Loss: 0.49655669927597046, Train Accuracy: 84.40%, Test Accuracy: 85.43%\n",
      "Epoch [8/10], Loss: 0.4935138002038002, Train Accuracy: 84.10%, Test Accuracy: 86.67%\n",
      "Epoch [9/10], Loss: 0.45682963728904724, Train Accuracy: 85.60%, Test Accuracy: 86.67%\n",
      "Epoch [10/10], Loss: 0.45345984399318695, Train Accuracy: 85.50%, Test Accuracy: 87.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-23 17:01:29,761] Trial 4 finished with value: 87.82 and parameters: {'lambda_kmeans': 0.018819718238073685, 'lambda_consistency': 6.692366450568696e-05}. Best is trial 2 with value: 89.46.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.297993779182434, Train Accuracy: 20.20%, Test Accuracy: 47.11%\n",
      "Epoch [2/10], Loss: 1.8772067725658417, Train Accuracy: 46.20%, Test Accuracy: 70.81%\n",
      "Epoch [3/10], Loss: 1.3026941120624542, Train Accuracy: 73.90%, Test Accuracy: 70.81%\n",
      "Epoch [4/10], Loss: 0.8098151981830597, Train Accuracy: 78.30%, Test Accuracy: 83.58%\n",
      "Epoch [5/10], Loss: 0.5619940608739853, Train Accuracy: 82.90%, Test Accuracy: 83.58%\n",
      "Epoch [6/10], Loss: 0.4932525083422661, Train Accuracy: 84.00%, Test Accuracy: 86.27%\n",
      "Epoch [7/10], Loss: 0.4561535269021988, Train Accuracy: 85.60%, Test Accuracy: 86.27%\n",
      "Epoch [8/10], Loss: 0.44601666927337646, Train Accuracy: 85.30%, Test Accuracy: 87.76%\n",
      "Epoch [9/10], Loss: 0.4311358481645584, Train Accuracy: 85.60%, Test Accuracy: 87.76%\n",
      "Epoch [10/10], Loss: 0.432711124420166, Train Accuracy: 86.40%, Test Accuracy: 88.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-23 17:03:10,463] Trial 5 finished with value: 88.56 and parameters: {'lambda_kmeans': 5.539315622786286e-05, 'lambda_consistency': 0.00013725499218143717}. Best is trial 2 with value: 89.46.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.2072219848632812, Train Accuracy: 22.10%, Test Accuracy: 50.36%\n",
      "Epoch [2/10], Loss: 1.7091401517391205, Train Accuracy: 60.50%, Test Accuracy: 78.29%\n",
      "Epoch [3/10], Loss: 1.0706259906291962, Train Accuracy: 75.30%, Test Accuracy: 78.29%\n",
      "Epoch [4/10], Loss: 0.710592195391655, Train Accuracy: 77.80%, Test Accuracy: 84.79%\n",
      "Epoch [5/10], Loss: 0.5260713547468185, Train Accuracy: 83.50%, Test Accuracy: 84.79%\n",
      "Epoch [6/10], Loss: 0.4239533469080925, Train Accuracy: 85.60%, Test Accuracy: 87.02%\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    lambda_kmeans = trial.suggest_loguniform('lambda_kmeans', 1e-5, 1e-1)\n",
    "    lambda_consistency = trial.suggest_loguniform('lambda_consistency', 1e-6, 1e-2)\n",
    "    # dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
    "    \n",
    "    # Use a fixed value of M for the optimization\n",
    "    M = 1000  # You can change this value as needed\n",
    "    values_of_M = [M]\n",
    "    \n",
    "    # Run the experiment with the current hyperparameters\n",
    "    results = experiment(\n",
    "        values_of_M=values_of_M, \n",
    "        epochs=10, \n",
    "        evaluate_every=2,\n",
    "        lambda_kmeans=lambda_kmeans, \n",
    "        lambda_consistency=lambda_consistency, \n",
    "        use_consistency=True, \n",
    "        use_unlabeled=True, \n",
    "        generate_tsne=False, \n",
    "        generate_cm=False\n",
    "    )\n",
    "    \n",
    "    # Get the test accuracy for the current hyperparameters\n",
    "    test_accs = results[M][1]\n",
    "    best_test_acc = max(test_accs)\n",
    "    \n",
    "    return best_test_acc\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "set_seed(7 * 5 * 3 * 2 * 2)\n",
    "\n",
    "study_name = \"Optimize_KMeans_and_Consistency_Losses\"\n",
    "study = optuna.create_study(study_name=study_name, direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Visualize Hyperparameter Optimization Results </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization History Plot\n",
    "history_plot = optuna.visualization.plot_optimization_history(study)\n",
    "history_plot.show()\n",
    "\n",
    "# Hyperparameter Importance Plot\n",
    "importance_plot = optuna.visualization.plot_param_importances(study)\n",
    "importance_plot.show()\n",
    "\n",
    "# Parallel Coordinate Plot\n",
    "parallel_plot = optuna.visualization.plot_parallel_coordinate(study)\n",
    "parallel_plot.show()\n",
    "\n",
    "# Slice Plot\n",
    "slice_plot = optuna.visualization.plot_slice(study)\n",
    "slice_plot.show()\n",
    "\n",
    "# Contour Plot\n",
    "contour_plot = optuna.visualization.plot_contour(study)\n",
    "contour_plot.show()\n",
    "\n",
    "# EDF Plot\n",
    "edf_plot = optuna.visualization.plot_edf(study)\n",
    "edf_plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
